[
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "How a Customer Reduced Total Cost of Ownership (TCO) by 28% for Storage with Amazon FSx for NetApp ONTAP by Sachin Bawse and Vishnu Vashist | on September 08, 2025 | in Advanced (300), Amazon FSx for NetApp ONTAP, Best Practices, Healthcare, Technical How-to | Permalink\nOrganizations with multiple branch offices often face significant challenges in managing distributed file systems, especially when relying on traditional on-premises infrastructure. Maintaining smooth file sharing across geographically dispersed locations—while ensuring security, efficient data management, and reliable user authentication—is becoming increasingly complex in today’s digital landscape.\nAmazon FSx for NetApp ONTAP addresses these challenges by providing a fully managed cloud-native file storage solution, offering high-performance storage with built-in replication, automated synchronization, and intelligent caching.\nIn this article, the authors share how a customer deployed FSx for ONTAP with a Multi-AZ configuration, using NetApp FlexCache for local caching and conducting data migration using SnapMirror. During this process, they completed performance tests, analyzed architectural trade-offs, and performed a cost comparison—showing a 28% reduction in total cost of ownership (TCO) compared to a traditional on-premises solution.\nSolution Overview The core architecture centers around deploying an Amazon FSx for NetApp ONTAP Multi-AZ cluster spanning two Availability Zones to ensure high availability and fault tolerance.\nBranch locations connect to FSxN through secure VPN connections, accessing data via ONTAP FlexCache volumes deployed on-premises or within VMware environments. These caches help reduce bandwidth usage and improve responsiveness by serving frequently accessed data locally.\nData migration is performed using NetApp SnapMirror, ensuring consistent data replication from the on-prem environment to AWS.\nFigure 1: Distributed file system with Amazon FSx or NetApp ONTAP architecture.\nCommunication and caching layers include:\nLocal branch → FlexCache volumes (serving hot data) FlexCache → FSx ONTAP origin cluster across AZs SnapMirror → replicating data to FSx, ensuring storage efficiency The customer also conducted performance comparisons: when transferring a 50 MB file, ONTAP Select cache connected to FSx achieved 26.09 seconds, compared to 24.20 seconds on a local file server, demonstrating near on-prem performance.\nLatency and FlexCache Write-Back Considerations The write-back feature of FlexCache is particularly helpful when latency between the branch cache and the origin cluster exceeds 8 ms. Under these conditions, caching significantly improves write performance.\nKey architectural requirements include:\nCPU \u0026amp; RAM: Each node in the origin cluster requires at least 128 GB RAM and approximately 20 vCPUs to support write-back workloads. ONTAP Version: Both the origin cluster and the cache must run ONTAP 9.15.1 or later to enable write-back. Licensing: FlexCache (including write-back) is built-in; no additional license is needed. Cluster Peering: The origin and cache clusters must be peered, and their storage virtual machines (SVMs) must also be vserver-peered with FlexCache enabled. These principles ensure cache stability in distributed branch deployments.\nMonitoring and Visibility To gain deeper insight into FSx \u0026amp; ONTAP performance beyond default Amazon CloudWatch metrics, the solution uses NetApp Harvest with Grafana.\nHarvest collects metrics such as:\nperformance cache hit ratio storage efficiency volume-level statistics This provides administrators with detailed observability and proactive optimization capabilities.\nTCO Estimate Compared to On-Premises To quantify the cost benefits, the customer modeled a hybrid scenario comparing FSx for ONTAP (Multi-AZ, SSD + capacity tier at ~30/70 ratio, throughput ~256 MB/s) to an equivalent on-prem system.\nResults showed that FSx for ONTAP reduces TCO by an estimated 28%, driven by lower operational overhead, improved storage efficiency, and simplified infrastructure management.\nConclusion Amazon FSx for NetApp ONTAP is a powerful solution for distributed file systems, especially suited for multi-branch architectures.\nThrough Multi-AZ deployment, FlexCache for local acceleration, SnapMirror for migration, and detailed monitoring, this architecture delivers near-local performance, high availability, and a 28% reduction in TCO compared to on-premises solutions.\nThis deployment demonstrates how modern cloud-integrated storage addresses traditional limitations of bandwidth, consistency, and operational complexity across multi-site environments.\nAuthors Sachin Bawse Sachin is a GTM Specialist Storage Solution Architect at Amazon Web Services (AWS), where he focuses on storage optimization, data migration, and performance improvement for customer workloads. Outside work, Sachin is passionate about traveling, exploring new destinations, discovering different cultures, and enjoying diverse cuisine. Vishnu Vashist Vishnu Vashist is a Partner Success Solutions Architect in the AWS Partner Organization, supporting customers across Healthcare \u0026amp; Life Sciences (HCLS) as well as Travel, Transportation, and Logistics. He specializes in migration and modernization, supporting large-scale AWS migration projects and guiding customers and partners on architecture and infrastructure design. "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Automating vector embedding generation in Aurora PostgreSQL with Bedrock by Domenico di Salvia and Andrea Filippo La Scola | on September 05, 2025 | in Advanced (300), Amazon Aurora, Amazon Bedrock, Technical How-to | Permalink\nVector embeddings have significantly transformed the way we interact with unstructured data in generative AI applications. Embeddings are mathematical representations that enable semantic search, recommendation systems, and several natural language processing tasks by capturing the essence of text, images, and other content in a machine-processable form. In applications using Retrieval-Augmented Generation (RAG) or similar AI solutions, keeping embeddings up to date as data changes is crucial to ensure search and recommendation results remain relevant.\nAlthough Bedrock provides fully managed RAG solutions, many organizations have specific requirements that lead them to build custom vector database solutions using PostgreSQL along with the pgvector extension. In this article, the authors present multiple approaches for automatically generating embeddings in Aurora PostgreSQL when data is inserted or updated. Each approach makes trade-offs in complexity, latency, reliability, and scalability so you can choose the strategy that best fits your application.\nSolution Overview When using Aurora PostgreSQL with the pgvector extension to build a vector database, you need a reliable mechanism to generate or update embeddings whenever the underlying data changes. The general workflow is:\nDetect when new or modified data requires an embedding. Send that content to a Bedrock embedding model (e.g., Titan). Receive the vector embedding. Store it alongside the source data. In the article, the authors use Titan as the underlying embedding model via Bedrock because it provides production-grade embeddings without managing additional infrastructure. You may also choose other supported models (e.g., Cohere Embed, Anthropic Claude) or even custom models via SageMaker or open-source libraries.\nPrerequisites Before implementing any approach, ensure you have:\nAn Aurora PostgreSQL cluster with pgvector enabled. IAM roles \u0026amp; policies configured to allow Bedrock access. For approaches using AWS Lambda, VPC configuration allowing Lambda to access both the database and Bedrock. For the aws_ml extension, a compatible database version. For pg_cron, ensure the extension is enabled and configured. The authors provide a GitHub repository with an AWS CDK stack and source code to deploy the demo environment. Implementation Approaches The article describes five automation strategies, each with pros and cons.\n1. Database triggers + aws_ml extension (synchronous) Trigger inside the database calls aws_ml synchronously within the same transaction to generate embeddings. Includes sample PL/pgSQL code for generating and storing embeddings. Pros: immediate consistency, few external components. Cons: slows down transactions, limited scalability, complex error handling, risk of timeouts. 2. Database triggers + aws_lambda extension (synchronous) Trigger synchronously invokes a Lambda function, which generates and returns the embedding. Logic is separated from the database but still synchronous. Pros: more flexibility in Lambda (pre/post-processing), better error observability. Cons: still blocks the transaction, Lambda latency (cold starts), more configuration overhead. 3. Database triggers + aws_lambda extension (asynchronous) Trigger invokes Lambda asynchronously, returning immediately without waiting for the embedding. Lambda later writes the embedding into a document_embeddings table (e.g., via RDS Data API). Pros: transactions are not blocked; improved write throughput. Cons: eventual consistency; more complex error handling; slower embedding generation. 4. SQS queue + Lambda batch processing (asynchronous) Trigger sends data changes into an SQS queue; a Lambda consumer processes items in batches. Batching reduces API calls, increases retries, and improves fault tolerance. Pros: highly scalable, robust error handling, efficient API usage. Cons: longer delay between data changes and embedding availability; more operational complexity. 5. Scheduled updates using pg_cron extension (asynchronous) pg_cron schedules periodic jobs to fetch records requiring embeddings, process them in batches, and update results. No triggers, so initial data writes are unaffected. Pros: simple architecture, fewer external dependencies. Cons: delayed embedding generation, batch overhead, potential database load during cron runs. Design Factors to Consider When selecting a production strategy, consider:\nBedrock API rate limits – heavy usage may require batching or throttling. Token limits – long text may need chunking. Costs – dependent on Lambda invocations, API usage, and other AWS services. Latency requirements – each method has different delays. Database write performance – synchronous methods slow down transactions. Error handling – asynchronous architectures handle retries more robustly. Index maintenance – vector indexes degrade over time and require periodic optimization. Decision Tree The authors provide a decision diagram to help choose an embedding strategy based on application needs (latency, consistency, complexity). Start with simpler approaches (e.g., option 1 or 5) and evolve as requirements grow.\nThey also note the need for vector index maintenance — depending on the index type, periodic maintenance is required to preserve search quality and performance.\nA complete solution, deployment scripts, and sample code are available in the GitHub repository linked in the article.\nCleanup To avoid unnecessary costs, the authors recommend:\nDeleting all CloudFormation stacks used for the demo. Removing any additional AWS resources created during testing. Conclusion Automating embedding generation in Aurora PostgreSQL ensures that AI capabilities such as semantic search, recommendations, and data retrieval stay up to date as your data evolves. Keeping embeddings synchronized with data changes maintains relevance and accuracy for AI applications.\nThe article outlines a wide range of methods—from triggers, cron jobs, and queues to batch solutions—each with trade-offs around latency, scalability, consistency, and operational complexity. The best choice depends on your application’s tolerance for delay, write volume, and operational overhead.\nThe authors encourage reviewing the accompanying GitHub repository for full implementation details and example code. Feedback and contributions via issues or pull requests are welcome.\nAuthors Andrea Filippo La Scola Andrea is a Partner Solutions Architect at AWS specializing in data analytics and serverless architectures. He supports AWS partners and customers in Italy in designing innovative solutions based on AWS services. Domenico di Salvia Domenico is a Senior Database Specialist Solutions Architect at AWS. He works with customers across the EMEA region, providing technical guidance for database projects. He helps them maximize value when using or migrating to AWS by designing cloud database architectures that are scalable, secure, high-performance, sustainable, cost-efficient, and reliable. "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Group database tables under AWS Database Migration Service tasks for PostgreSQL source engine by Manojit Saha Sardar and Chirantan Pandya | on September 05, 2025 | in Amazon Aurora, AWS Database Migration Service, Expert (400), PostgreSQL compatible, RDS for PostgreSQL, Technical How-to | Permalink\nIn large-scale data migration projects using AWS DMS (Database Migration Service), properly grouping source tables into tasks is crucial for ensuring high performance and avoiding latency during the full load or CDC phases. In this article, the authors demonstrate how to analyze the PostgreSQL source database to determine optimal task sizing and table grouping, helping you plan the migration in a way that minimizes latency and maximizes throughput.\nBackground \u0026amp; Motivation During migration, certain tasks may run slowly or become bottlenecks due to suboptimal grouping — for example, combining too many small tables or mixing extremely large tables with smaller ones.\nDelays can arise during both full load and CDC because of resource contention, I/O bottlenecks, or uneven load distribution.\nBy analyzing table characteristics and system metrics, you can make well-informed decisions regarding the number of DMS tasks, grouping strategy, and identifying tables that should be isolated.\nSolution Overview The approach combines metadata from the source database (catalogs, statistical views) with information about hardware and workload to:\nDetermine the appropriate number of DMS tasks Group tables into balanced tasks Isolate “special” tables (for example: very large tables, tables with LOBs) to avoid cross-impact Proposed workflow:\nCreate a control table on the source database Populate metadata (size, partitions, indexes, LOB presence, DML statistics) Monitor daily changes/growth Classify tables by step/priority Group tables into tasks Execute migration based on these groups This method helps minimize latency and provides a more accurate estimation of task size.\nThe following diagram illustrates the solution architecture.\nPrerequisites You need:\nKnowledge of AWS DMS (Database Migration Service) A PostgreSQL source database and the ability to run SQL / PL/pgSQL scripts to collect metadata Step 1: Create the control table On the PostgreSQL source database, create a table (for example, TABLE_MAPPING) to store metadata for each candidate table:\nCREATE TABLE TABLE_MAPPING ( OWNER VARCHAR(30), OBJECT_NAME VARCHAR(30), OBJECT_TYPE VARCHAR(30), SIZE_IN_MB NUMERIC(12,4), STEP INTEGER, IGNORE CHAR(3), PARTITIONED CHAR(3), PART_NUM INTEGER, SPECIAL_HANDLING CHAR(3), PK_PRESENT CHAR(3), UK_PRESENT CHAR(3), LOB_COLUMN INTEGER, GROUPNUM INTEGER, TOTAL_DML INTEGER );\nThis table stores one row for each table (or each partition), containing metrics such as table size, number of partitions, presence of PK/UK, number of LOB columns, total DML, and more.\nStep 2: Populate the control table Use PostgreSQL system catalogs and statistical views (such as pg_tables, pg_partitioned_table, pg_inherits, and statistical views) to collect:\nTable size and partitioning information Number of indexes, and whether primary / unique keys exist Number of LOB columns Amount of DML (insert / update / delete) Partition statistics (min, max, average size) This provides a metadata + workload “snapshot” to guide your table grouping decisions.\nStep 3: Monitor the change level Track the amount of DML activity on each table over time. This helps determine which tables are “hot” (frequently changing) and which are more static, which should be taken into account during grouping.\nStep 4: Classify tables / assign step numbers Based on table size, change frequency, special handling needs (LOB, missing PK), or partitioning, assign a step number or priority to each table. For example:\nStep 1: small tables with low change rate Step 2: medium tables with moderate change Step N: very large tables or tables with heavy change activity You can also mark IGNORE for tables you do not want included in a specific task, or SPECIAL_HANDLING for tables requiring isolation or special treatment.\nStep 5: Group tables into tasks Use the control table and the classification to group tables so that:\nEach task has a balanced workload (size + change rate) Very large tables (e.g., \u0026gt; 2 TB) can be isolated Tables with LOBs or missing PK should be grouped carefully or isolated High-change tables can be separated to avoid affecting others The total number of tasks should not be excessive and should match the capacity of the replication instance The goal is to reduce lag, minimize resource contention, and evenly distribute workload across tasks.\nFactors to consider When grouping and sizing tasks, consider:\nDatabase object size: extremely large tables should be isolated or handled separately Partitioned vs non-partitioned: partitioned tables may allow parallel migration Presence of PK / unique index: DMS requires PK/UK for LOB handling and avoiding duplicates Use of LOBs: LOB-heavy tables increase complexity and migration cost — isolating them is recommended Change volume (CDC): high-change tables should be grouped carefully to avoid CDC lag Parallelism \u0026amp; resource limits: replication instance capacity (CPU, I/O), network bandwidth, etc., determine the optimal number of tasks Reference architecture \u0026amp; workflow The article includes a diagram showing how source tables are classified, grouped, and processed in parallel migration tasks (not shown here).\nIt also describes how the control table is maintained during migration and used to track progress and adjust groupings as needed.\nSummary \u0026amp; recommendations By:\nSystematically analyzing metadata Classifying and grouping tables based on characteristics and workload Isolating high-risk tables (very large, LOB, missing PK) Balancing load across tasks You can reduce migration risks, improve task size estimation, and achieve a smooth, high-performance migration.\nAuthors Manojit Saha Sardar Manojit is a Senior Database Engineer at AWS and is recognized as an expert in AWS DMS, Amazon RDS, and Amazon RDS for PostgreSQL. In his role at AWS, he collaborates with customers to resolve various data transfer scenarios and supports challenges involving Amazon RDS for Oracle and Amazon RDS for PostgreSQL. Chirantan Pandya Chirantan is a Database Engineer (AWS Countdown Premium) and an expert in AWS DMS and Amazon RDS for PostgreSQL. At AWS, he works closely with customers to provide guidance and technical support for database migration projects, as well as projects involving Amazon RDS for PostgreSQL and Oracle. "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Create Knowledge Base",
	"tags": [],
	"description": "",
	"content": "What is Knowledge Base ? Knowledge Base là gì ? Knowledge Base is \u0026ldquo;private memory\u0026rdquo; for Agent to access your specialized information, which the original AI model does not know. On the interface page of Aws Bedrock, I choose the left side Knowledge Base When you have finished configuring, wait about 5 minutes for it to initialize. This has created the Knowledge base. Note that you should configure it according to your needs and grant it the necessary permissions\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “Cloud Day” Event Objectives Introduce the trend of AI development in Vietnam and its economic potential. Present the evolution from Generative AI to Agentic AI. Showcase AWS solutions like Amazon Bedrock, AgentCore, and SageMaker Unified Studio for building, deploying, and operating AI agents. Speakers Kien Nguyen - Solutions Architect Jun Kai Loke - AI/ML Specialist SA, AWS Tamelly Lim - Storage Specialist SA, AWS Binh Tran - Senior Solutions Architect, AWS Taiki Dang - Solutions Architect, AWS Michael Armentano - Principal WW GTM Specialist, AWS Key Highlights Impact of AI on Vietnam\u0026rsquo;s Economy AI could contribute USD 120–130 billion to Vietnam’s GDP by 2040 (~25%). The AI market is valued at USD 750 million, growing 15–18% annually. Vietnam has 765 AI startups, ranking 2nd in ASEAN. Great potential, but still in an early stage — requires infrastructure, talent, and policy development. Evolution: From AI to Agentic AI Generative AI Assistants → Generative AI Agents → Agentic AI Systems. AI systems are becoming less dependent on human supervision. Multi-agent systems allow agents to collaborate to solve complex tasks. The trend is toward higher automation and lower human intervention. Applications of Agentic AI in Organizations Improve workplace productivity, automate workflows, and drive innovation. By 2028, 33% of enterprise applications will integrate Agentic AI. About 15% of daily business decisions will be automated through AI agents. Amazon Bedrock – The All-in-One AI Platform Offers diverse foundation models from leading providers. Enables customization with private data, ensuring security and cost control. Integrates Responsible AI checks for safe and ethical use. Supports fast, scalable, and secure agent deployment and management. Amazon Bedrock AgentCore A secure, scalable environment for building and running AI Agents. Supports frameworks such as LangChain, CrewAI, LangGraph, and Strands Agents. Manages short-term and long-term memory and supports semantic search. Allows easy integration and tool discovery for developers. Data \u0026amp; AI Infrastructure Introduces Amazon SageMaker Unified Studio – the central hub for data, analytics, and AI development. Integrates tightly with: Amazon Redshift, Athena, EMR, Glue – for data processing and storage. Amazon QuickSight – for data visualization. Amazon Bedrock – for GenAI development. Supports Zero-ETL integration between S3 data lakes and Redshift data warehouses. Data Lakehouse Concept Supports multiple storage layers: S3 Tables, Redshift Managed Storage. Connects various data sources: Aurora, DynamoDB, MSK, Kinesis, OpenSearch, Salesforce, SAP, Facebook Ads, and more. Key Takeaways On AI and Cloud Mindset Understood that Agentic AI is the next evolution of Generative AI. Realized that AI Agents are not just chatbots but systems capable of taking autonomous actions and decisions. Learned how AWS Bedrock provides a foundation for building enterprise-level AI systems. Recognized the strategic value of AI agents in enterprise automation and innovation. On Technical Architecture Learned the relationships between Bedrock – SageMaker – Redshift – S3 in a unified AI ecosystem. Understood how AWS manages memory, tool discovery, and observability for AI agents. Applying to Work Apply Amazon Berock to an existing project: Use Amazon Titan Embeddings to create an embedding Experiment with Zero-ETL integration between Amazon Redshift and Aurora/DynamoDB. Evaluate Amazon Bedrock AgentCore for automating workflows with intelligent agents (beyond Lambda-based implementations). Event Experience Attending Cloud Day was an incredibly insightful experience that gave me a clearer view of how businesses leverage AI to modernize systems and enhance productivity.\nLearning from Experts AWS experts deeply explained Agentic AI and how it differs from traditional Generative AI. Through Amazon’s real-world examples, I gained a deeper understanding of multi-agent system design and workflow optimization. Hands-on Technical Insights Explored the technical operation of Amazon Bedrock AgentCore, including memory handling and tool integration. Observed how S3 – Redshift – SageMaker integrate seamlessly for contextual AI data retrieval. Learned the fundamentals of Lakehouse architecture and Zero-ETL data flow in practice. Using Modern Tools Discovered how to quickly deploy Agentic AI systems on AWS Bedrock with security, scalability, and reliability. Lessons learned Agentic AI represents a strategic step toward full enterprise automation. Modern AI infrastructure must be built on data-driven and cloud-native architecture. AWS is leading the ecosystem with comprehensive tools such as Bedrock and SageMaker. Understood the importance of AI agents in enhancing both automation and innovation in modern businesses. Some event photos Overall, the event not only provided valuable technical knowledge but also reshaped my mindset on system modernization, design thinking, and effective collaboration between teams.\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Report “Generative AI with Amazon Bedrock” Purpose of the Event Provide foundational knowledge about Generative AI and how it differs from traditional Machine Learning. Detailed description of Amazon Bedrock and Foundation Models. Technical guidance on RAG (Retrieval Augmented Generation) to build intelligent and accurate AI applications while reducing hallucinations. Introduction to the ecosystem of specialized AI services on AWS. Speakers Lam Tuan Kiet - Sr DevOps Engineer, FPT Software. Danh Hoang Hieu Nghi - AI Engineer, Renova Cloud. Dinh Le Hoang Anh - Cloud Engineer Trainee, First Cloud AI Journey. Key Highlights The shift: Traditional ML vs Foundation Models Traditional ML Models: Specialized for specific tasks, require labeled data, and need separate training/deployment pipelines for each purpose. Foundation Models (FM): Trained on massive amounts of unstructured data, capable of adapting to various tasks such as text generation, summarization, Q\u0026amp;A, and chatbot applications. The AI Ecosystem on AWS Amazon Bedrock: A hub of leading foundation models from AWS partners (AI21 Labs, Anthropic, Cohere, Meta, Stability AI,\u0026hellip;) and Amazon models. AWS Specialized AI Services: “Instant-use” AI services optimized for specific tasks without requiring model training: Amazon Rekognition: Object detection, face recognition, emotion detection, celebrity recognition, video analysis – $0.001/image for the first 1M images. Amazon Translate: Real-time multilingual text translation with high accuracy and natural tone. Amazon Textract: Extracts structured information (tables, forms) from scanned documents or PDFs. Amazon Transcribe: Converts speech to text. Amazon Polly: Converts text to speech. Amazon Comprehend: Sentiment analysis, keyword extraction, and topic classification. Amazon Kendra: Natural-language search across internal enterprise documents. Amazon Lookout: Detect anomalies in production lines or industrial machinery for predictive maintenance. Amazon Personalize: Real-time recommendation system powered by machine learning. Prompting Technique: Chain of Thought (CoT) Comparison between Standard Prompting and Chain-of-Thought Prompting. CoT guides the model to reason step-by-step for complex logic problems, significantly improving accuracy compared to simply requesting the final result. RAG (Retrieval Augmented Generation) – Technical Core Problem: Addressing hallucinations and lack of updated knowledge in LLMs. Solution: Combine Retrieval from an external Knowledge Base with the Generation capability of LLMs. Data Ingestion Process: Raw Data (New data) → Chunking. Processed through an Embeddings model (e.g., Amazon Titan Text Embeddings V2.0). Stored as vectors in a Vector Store (OpenSearch Serverless, Pinecone, Redis\u0026hellip;). RetrieveAndGenerate API: Manages the entire pipeline from user input → embedding query → retrieving relevant data → augmenting the prompt → generating the final response. What I Learned AI and Cloud Mindset Understand when to use Specialized AI Services for fast, specific tasks and when to use Bedrock/GenAI for creative and complex workloads. Master the design mindset of RAG systems: not just calling LLM APIs, but managing data and vectorization to provide accurate context for AI outputs. Technical Architecture The Chain of Thought technique is key to optimizing model output without fine-tuning. Deep understanding of Amazon Titan Embeddings V2.0 and its role in converting multilingual text into vectors (supports 100+ languages, flexible vector sizes 256/512/1024). Application to Work Applying Amazon Bedrock to the current project: Amazon Rekognition: Identify food items from images to auto-fill calorie information. Amazon Comprehend: Analyze text to standardize food names and store calorie data. Experiment with RAG in the ongoing project. Use Bedrock Agents to orchestrate tasks such as querying food items from a vector store, calculating calorie goals, and generating daily meal plans. Experience at the Event Joining the workshop “Generative AI with Amazon Bedrock” provided a very practical perspective on building modern AI applications, from foundational theory to real-world implementation.\nHands-on Knowledge from Experts Speakers clearly explained the data flow in a RAG system, helping me visualize the “black box” behind modern chatbot applications. The distinction between Traditional ML and Generative AI helped reshape my strategy for choosing technologies in future projects. Technology Experience Impressed by RetrieveAndGenerate API in Bedrock as it eliminates much manual work in linking Vector Stores with LLMs. Saw the power of Amazon Titan Embedding in supporting multilingual applications, making it very suitable for the Vietnamese market. Key Takeaways RAG is the new standard: For AI to work in enterprises, RAG is essential for accuracy and data security. Full ecosystem: AWS provides everything from infrastructure (Vector Store) to models (Bedrock) and application-level orchestration (Agents), significantly accelerating implementation. Some Photos from the Event Overall, the event not only delivered technical knowledge but also helped reshape my mindset about application design, system modernization, and effective collaboration among teams.\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/5-workshop/5.3-vpc-to-internet/5.3.1-create-gwe/",
	"title": "Exploring Bedrock",
	"tags": [],
	"description": "",
	"content": " Search Amazon Bedrock pricing I consider the price of each model and domain to see if it suits my needs Open AWS Console → Amazon Bedrock Select the model catalog, choose any model to test to suit your purpose and price I will choose Amazon Nova 2 lite here because it is cheap ($0.0003/per 1000 tokens) Select Open in Playground Try the prompt and get the result The model results are quite impressive. "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyen Quoc Bao\nPhone Number: 0969197845\nEmail: baonqse181700@fpt.edu.vn\nUniversity: FPT University\nMajor: Artificial Intelligence (AI)\nStudent ID: SE181700\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "About Amazon Bedrock Amazon Bedrock is a fully managed machine learning service from AWS, providing access to leading Foundation Models from Anthropic (Claude), Amazon (Titan), Meta (Llama), and many others through a simple API.\nAnthropic Claude - Claude 2, Claude 3 Meta Llama - Llama 2 Amazon Titan - Titan Text AI21 Labs - Jurassic-2 Workshop Overview In this workshop, you will learn how to:\nExplore Bedrock Console\nCreate AI Agent with AWS Lambda\nExpose API to external applications\nTest with a simple web interface\nImage source\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Goals: Connect and get acquainted with members of the First Cloud Journey. Understand basic AWS services and how to create and manage costs in an AWS account. Learn how to use AWS Console \u0026amp; AWS CLI to interact with and manage services. Tasks to be performed this week: Day Task Start Date Completion Date Reference 2 - Get acquainted with FCJ members - Read and take note of the rules and regulations of the internship program 09/08/2025 09/08/2025 3 - Learn about AWS and basic service categories + Compute (EC2) + Storage (S3) + Networking (VPC) + Database (RDS) 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Manage identities and permissions + Install \u0026amp; configure AWS CLI + Use AWS CLI for basic operations 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn how to manage AWS costs effectively with AWS Budget + Budget + Cost Budget + Usage Budget + Reservation (RI) Budget + Savings Plans Budget - Practice: + Create Cost Budget + Create Usage Budget + Create RI Budget + Create Savings Plans Budget + Clean up resources 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about AWS Support services - AWS Support plans + Basic, Developer, Business, Enterprise - Types of support cases + Account \u0026amp; Billing Support + Service Quota Increase + Technical Support - Practice: + Select Basic Support plan + Create a support case 09/12/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and gained knowledge of basic service groups:\nCompute: Provides processing resources such as virtual machines, containers, etc. Storage: Used for storing data, backup, and recovery. Networking: Manages networking infrastructure, security, and connections between AWS resources. Database: Provides relational and non-relational managed database services. Successfully created, configured, and identified AWS Free Tier account.\nLearned to create and manage User Groups and Users.\nAble to log in using IAM, and understand that users in the same group share assigned permissions.\nBecame familiar with AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nChecking account \u0026amp; configuration information Listing regions Creating and deleting S3 buckets Using Amazon SNS Creating IAM groups, users, and adding users Creating and deleting access keys Creating and basic configuration of VPS Launching and terminating EC2 instances Learned how to manage and monitor AWS costs using:\nCreating and configuring Budget plans (Cost, Usage, RI, Savings Plan) Understanding how to clean up resources to prevent unnecessary costs Understood AWS Support plans and learned how to create support cases.\nBasic: Free, supports account and billing issues via the help center Developer: $29/month, provides basic architectural guidance and unlimited technical support from the root account Business: $100/month, the popular choice for SMBs with features such as use-case guidance, access to AWS Support API, unlimited support cases for all IAM users, etc. Enterprise: $15,000/month, for large enterprises with the highest security, architectural, infrastructure support, strategic guidance, cost optimization, and priority handling. Became familiar with AWS Console and was able to perform basic operations efficiently through both the Console and CLI.\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Goals: Design and deploy a VPC following the AWS Well-Architected Framework. Configure key network security components. Establish secure connectivity between on-premises environments and AWS. Tasks to be performed this week: Day Task Start Date Completion Date Reference 2 - Learn about Amazon Virtual Private Cloud (Amazon VPC), Architecture and Scope + AWS Region + Availability Zones (AZ) + Classless Inter-Domain Routing (CIDR) - Learn about core Amazon VPC components + Subnet + Route Table + Internet Gateway + NAT Gateway - Learn about VPC firewall components + Security Group + Network ACLs + How to use the Resource Map - Practice: + Create VPC + Create Subnets + Create Internet Gateway + Create Route Table + Create Security Group + Enable VPC Flow Logs 09/15/2025 09/15/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn EC2 basics + Instance types + AMI + Key Pair + Network Setup - Learn SSH remote methods into EC2 - Learn Elastic IP - Learn VPC Reachability Analyzer - Learn AWS Systems Manager Session Manager and Amazon CloudWatch 09/16/2025 09/16/2025 https://cloudjourney.awsstudygroup.com/ 4 - Practice: Deploy Amazon EC2 with multi-AZ, create NAT Gateways and Amazon CloudWatch for VPC + Launch EC2 instances in Public and Private Subnets + SSH connection + Create and configure NAT Gateway + Use Reachability Analyzer + Create EC2 Instance Connect Endpoint + Use Session Manager + Deploy CloudWatch monitoring for VPC resources + Resource cleanup: Terminate EC2 instances, delete NAT Gateway and Elastic IP Address, remove VPC Endpoints 09/17/2025 09/17/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn AWS Site-to-Site VPN + AWS Site-to-Site VPN + Virtual Private Gateway + Customer Gateway + AWS VPN Tunnel - Learn Transit Gateway - Learn VPC Peering 09/18/2025 09/18/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Configure Site-to-Site VPN: Create VPN environment and configure VPN connectivity + Configure VPC Peering + Configure Transit Gateway 09/19/2025 09/19/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Successfully created and configured a VPC following the AWS Well-Architected Framework:\nMain VPC with CIDR block (10.0.0.0/16) Two subnets: Public and Private across separate Availability Zones (multi-AZ) Fully configured Route Tables, Internet Gateway, and NAT Gateway ensuring Private Subnet instances access the internet securely Enabled VPC Flow Logs and monitored network traffic in CloudWatch Logs Able to deploy and manage EC2:\nLaunched EC2 instances in Public and Private Subnets, assigned Elastic IP to the public instance Accessed EC2 using SSH, Instance Connect, and Session Manager (no Public IP required) Monitored EC2 operations via CloudWatch (CPU, Network, Status Checks) Configured Security Group and Network ACLs for enhanced security:\nSecurity Group – instance-level protection Network ACL – subnet-level protection Understood CloudWatch Alarms to warn about abnormal CPU or network activity.\nLearned and practiced AWS Site-to-Site VPN to securely connect AWS with on-premises:\nCreated Virtual Private Gateway (VGW) and attached it to VPC Created Customer Gateway (CGW) simulating on-premises environment Established a stable IPSec VPN tunnel between AWS and CGW Practiced VPC Peering to connect two isolated VPCs within the same Region\nLearned and tested AWS Transit Gateway\nUnderstood and applied AWS Systems Manager for managing infrastructure without direct SSH access\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Goals: Understand the core virtual server service (Amazon EC2) and key compute-related components in AWS. Gain a clear understanding of the architecture, operating mechanisms, and management of virtual server services in AWS. Tasks to be performed this week: Day Task Start Date Completion Date Reference 2 - Learn about Amazon Elastic Compute Cloud (EC2): + Instance type + AMI, Backup, Key Pair + Elastic Block Store + Instance Store + User Data + Meta Data + EC2 Auto Scaling + EFS/FSx – Lightsail – MGN 09/22/2025 09/22/2025 https://cloudjourney.awsstudygroup.com/ 3 - Hands-on: + Create and connect to EC2 Instances on Windows and Linux + Create EC2 Snapshot (Backup) + Install applications on EC2 + Use Tags and Resource Groups for resource management + Limit resource usage with IAM + Clean up resources 09/23/2025 09/23/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about Amazon CloudWatch + Metrics + Logs + Alarms + Dashboard - Hands-on: + Configure CloudWatch Metrics for EC2 + Analyze CloudWatch Logs from EC2 applications and create Metrics Filters + Configure CloudWatch Alarms for alerts + Build CloudWatch Dashboard for EC2 monitoring + Clean up resources 09/24/2025 09/24/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about EC2 Auto Scaling + Auto Scaling Group + Scaling types: manual, scheduled, dynamic + Launch Template + Elastic Load Balancer - Hands-on: + Create a Launch Template + Create a Target Group + Create a Load Balancer + Create an Auto Scaling Group for flexible scaling + Verify behaviors of different scaling mechanisms + Clean up resources 09/25/2025 09/25/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about Amazon Lightsail + Lightsail Database + WordPress Instance + PrestaShop E-Commerce Instance + Akaunting Instance + Akaunting Instance - Hands-on: + Deploy different instances and databases: WordPress, PrestaShop E-Commerce, and Akaunting + Use Lightsail Load Balancer + Create Snapshot for backup and restore + Scale up to a larger instance + Clean up resources 09/26/2025 09/26/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Gained understanding of the architecture and operation of Amazon EC2 and differentiated between Instance Types (General Purpose, Compute Optimized, Memory Optimized, Storage Optimized).\nUnderstood how to use Amazon Machine Image (AMI), Key Pair, Elastic Block Store (EBS), and Instance Store for server configuration and management.\nLearned how to use User Data and Meta Data to automate configuration during EC2 Instance launch.\nUnderstood and practiced working with EFS/FSx for shared storage across instances.\nSuccessfully created and connected to EC2 Instances using both Windows and Linux.\nCreated and restored EC2 Snapshots (Backups).\nInstalled basic web applications on EC2 Instances.\nUsed Tags and Resource Groups to classify and manage resources effectively.\nConfigured IAM to restrict access and control resource usage.\nFully understood the components of Amazon CloudWatch: Metrics, Logs, Alarms, and Dashboard.\nConfigured CloudWatch Metrics for EC2 and created Metrics Filters from application logs.\nSet up CloudWatch Alarms to receive alerts when CPU or network traffic exceeds thresholds.\nBuilt CloudWatch Dashboards to summarize and monitor EC2 system status.\nGained full understanding of EC2 Auto Scaling mechanisms and scaling types:\nManual: manually adjust the number of EC2 instances when resource demand changes. Scheduled: automatically scale based on defined schedules, suitable for predictable traffic patterns to reduce cost. Dynamic: automatically scale out/in based on CloudWatch metrics such as CPU usage or network traffic. Learned how to configure and test Auto Scaling Group behavior under varying load.\nStudied and deployed different Lightsail Instances:\nWordPress Instance: basic CMS website. PrestaShop Instance: e-commerce platform. Akaunting Instance: financial and accounting management system. Used Lightsail Database for application data storage and Lightsail Load Balancer for higher availability and load distribution.\nLearned how to create Snapshots for backups and upgrade to larger instance sizes.\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Goals: Understand and apply AWS storage services such as S3, Storage Gateway, and the Snow Family. Master backup and data recovery mechanisms using AWS Backup and Disaster Recovery on AWS. Learn how to implement secure, scalable, and highly available storage solutions for application systems. Tasks to be performed this week: Day Task Start Date Completion Date Reference 2 - Learn about Amazon Simple Storage Service (Amazon S3) + S3 Bucket + S3 Object + Access Point + Storage Classes: Standard, Standard-IA, Intelligent-Tiering, Glacier + S3 Static Website \u0026amp; CORS + Access Control + Endpoint \u0026amp; Versioning + Object Key \u0026amp; Performance + Glacier Tiers: Expedited, Standard, Bulk - Hands-on: + Create an S3 Bucket + Upload data to S3 + Host a static website on S3: enable static website feature, configure Block Public Access, and make objects public + Test the website + Accelerate the website with CloudFront + Move and copy S3 objects to another region + Clean up resources 09/29/2025 09/29/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn how to use AWS Backup to create a backup plan for active AWS resources + AWS Backup architecture + Backup Plan + AWS SNS (Simple Notification Service) - Hands-on: + Create an S3 Bucket and deploy AWS Backup infrastructure + Create a Backup Plan + Configure Notifications + Verify backup operations + Clean up resources 09/30/2025 09/30/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about Snow Family: Snowball, Snowball Edge, Snowmobile - Learn about Storage Gateway + File Gateway + Volume Gateway + Tape Gateway - Learn about Disaster Recovery: Recovery Time Objective (RTO) and Recovery Point Objective (RPO) + Backup and restore + Pilot Light (Active-Standby) + Warm Standby (Low Capacity Active-Active) + Full Capacity (Active-Active) - Hands-on: + Create a Storage Gateway + Create File Shares + Connect File Shares from on-premises environment + Clean up resources 10/01/2025 10/01/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about AWS Import/Export - Hands-on: + Prepare a virtual machine on VMware Workstation + Import a VM into AWS: export VM from on-premises, upload to AWS, import as AMI, launch EC2 Instance from imported AMI + Export EC2 Instance to VM: configure ACL for S3 Bucket, export from EC2 instance, export from AMI + Clean up resources 10/02/2025 10/02/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about Amazon FSx - Hands-on: + Prepare practice environment + Create an SSD Multi-AZ file system + Create an HDD Multi-AZ file system + Create new file shares + Test performance + Monitor performance + Enable data deduplication + Enable shadow copies + Manage user sessions and file opens + Enable user storage quotas + Enable continuous share + Scale throughput capacity + Scale storage capacity + Clean up resources 10/03/2025 10/03/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Understood the structure and operation of S3 Bucket, Objects, Access Points, ACL/Policy, CORS, Versioning, Object Keys, and storage classes (Standard, IA, Intelligent-Tiering, Glacier).\nSuccessfully implemented:\nCreated buckets and uploaded objects Hosted a static website and accelerated it with CloudFront Configured public access and access control Performed cross-region object replication Understood and practiced hybrid storage integration from on-premises to AWS using Storage Gateway.\nGained detailed understanding of the Snow Family and its use cases for large-scale data migration.\nLearned and applied Amazon FSx: created Multi-AZ SSD/HDD file systems, managed shares, shadow copies, deduplication, monitoring, and scaled performance/storage.\nMastered AWS Backup \u0026amp; Disaster Recovery mechanisms:\nUnderstood AWS Backup architecture, Backup Vault, Backup Plan, and lifecycle behavior. Successfully configured Backup Plan, SNS, Notifications, automated backups, and tested restores. Learned Disaster Recovery details: RTO: Maximum acceptable downtime for recovery. RPO: Maximum acceptable data loss from the time an incident occurs. Pilot Light Warm Standby Multi-Site Active-Active Gained ability to deploy secure, scalable, and highly available storage solutions:\nApplied Versioning and Replication (S3) for high availability. Used Multi-AZ FSx for fault-tolerant storage architecture. Used CloudFront for global optimization of static content delivery. Understood DR models for fast system recovery. Learned how to scale FSx throughput and storage capacity. Improved hybrid storage deployment skills between on-premises and AWS:\nLearned VM Import/Export workflows: export VM from VMware, upload to S3, import as AMI, launch EC2, export EC2 back to VM format. Connected Storage Gateway File Shares to the on-premises environment. "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Understand security services on AWS and AWS core security principles. Manage identities, access rights, and secure user authentication on AWS. Protect and operate data using encryption and data management mechanisms. Tasks to be implemented this week: Day Task Start Date End Date Resources Mon - Learn about the Shared Responsibility Model + Responsibilities of \u0026ldquo;AWS\u0026rdquo; and the \u0026ldquo;Customer\u0026rdquo; + Security responsibilities for each type of service - Learn the basics of Amazon Identity and Access Management (IAM): + Root Account + AWS IAM + IAM Principals + IAM User + IAM Policy + IAM Role + IAM Permission Boundary. 10/06/2025 10/06/2025 https://cloudjourney.awsstudygroup.com/ Tue - Practice: + Create IAM Group, IAM User, IAM Role, and Assume Role + Create EC2 Admin User, RDS Admin User, and Admin Group + Configure IAM Role Condition + Create IAM Role with Admin rights + Create IAM User + Configure Switch Role + Restrict by IP or Time + Limit User permissions + Create restrictive policy + Create restricted IAM + Test restricted User + Resource cleanup 10/07/2025 10/07/2025 https://cloudjourney.awsstudygroup.com/ Wed - Learn how to use Tags and Resource Groups: + Tags + AWS Resource Groups - Practice: + Use Tags via Console: create EC2 with Tags, add or remove Tags, and create resources by Tag + Use Tags via CLI + Create Resource Group + Manage access to EC2 Resource Tag services with AWS IAM + Create IAM User, IAM Policy, and IAM Role + Switch Role + Check IAM Policy: Access to AWS Region allowed/denied, use Resource Tag with values not meeting conditions, modify Resource Tag on EC2 and check policy + Create IAM Policy, IAM Role and check IAM Role + Resource cleanup. 10/08/2025 10/08/2025 https://cloudjourney.awsstudygroup.com/ Thu - Learn the basics of Amazon Cognito: + User Pool + Identity Pool - Learn about AWS Organizations - Learn about AWS Identity Center (SSO) - Learn about AWS Key Management Service (KMS) - Learn about AWS Security Hub - Practice: + Activate Security Hub + Review each standard set + Resource cleanup. 14/09/2025 15/09/2025 https://cloudjourney.awsstudygroup.com/ Fri - Practice: + Use AWS SSO + Create AWS Account in AWS Organizations, setup Organization Unit + Setup AWS SSO and test + Encryption with AWS KMS + Create Policy and Role + Create Group and User + Create Key Management Service + Create bucket and upload data to Amazon S3 + Create AWS CloudTrail for logging and Amazon Athena for data querying + Test and share encrypted data on S3 + Resource cleanup 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in Week 5: Better understanding of the AWS security model:\nSecurity responsibilities vary by service type, for example: AWS is responsible for security of the cloud and is fully managed by AWS (infrastructure, hardware, etc.). Customers are responsible for security configuration in the cloud for their data/applications (service configuration, access rights, data, encryption\u0026hellip;). Recognized that IAM and KMS play a critical role in AWS security. Understood and realized how to use AWS IAM:\nGrasped concepts: Root Account, IAM User, IAM Group, IAM Role, IAM Policy, Permission Boundary. Know how to create and manage basic IAM, assign permissions based on the least privilege principle, and use Roles to enhance security. Know how to use Tags to control and manage access:\nKnow how to use Tags for basic services via Console and CLI. Created Resource Groups to group resources by tags. Tested denial cases due to wrong Region, wrong Tag, or unsatisfied IAM Policy conditions. Understood the role of AWS Cognito:\nUser Pool: Manage user accounts and the login process. Identity Pool: Grant temporary AWS credentials for applications to access AWS services. Know how to use AWS Organizations and Identity Center:\nKnow how to create OUs and grasp the principles of operating a multi-account environment such as: environment isolation, centralized management, and applying unified security policies. Know how to setup AWS SSO, create Permission Sets, and log in to child accounts from a single identity. Know how to use AWS KMS to encrypt and protect data.\nKnow how to activate and use AWS Security Hub to evaluate security standards, understand findings, and improve them.\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Understand AWS relational (SQL) and non-relational (NoSQL) data storage services based on storage needs. Know how to deploy, operate, and optimize databases on AWS. Understand Amazon ElastiCache for accelerating data access. Tasks to be implemented this week: Day Task Start Date End Date Resources Mon - Learn about Amazon RDS and Aurora + Engines: MySQL, PostgreSQL\u0026hellip; + Multi-AZ Availability + Read Replicas Performance + Automated Backups and Restoration 10/13/2025 10/13/2025 https://cloudjourney.awsstudygroup.com/ Tue - Practice: + Prepare environment for RDS instance: Create VPC, EC2, RDS Security Group, DB Subnet Group + Create EC2 Instance + Create RDS Database Instance in Private Subnet connected to EC2 + Deploy application + Automated Backup and Restore + Resource cleanup 10/14/2025 10/14/2025 https://cloudjourney.awsstudygroup.com/ Wed - Learn Amazon DynamoDB: NoSQL + Core components + Primary key and Sort key + Read/Write Capacity with On-demand or Provisioned - Practice: Use AWS Console and Cloudshell to: + Create table + Write, read, and update data + Query data + Create global secondary index + Query secondary index 10/15/2025 10/15/2025 https://cloudjourney.awsstudygroup.com/ Thu - Learn AWS SDK Python packages (for Python) + Botocore + Boto3 - Practice: + Configure AWS CLI with Boto3 library + Use Python to develop with DynamoDB: Create Table, write, read, update, delete data, Load sample data, Query, scan data, and delete table + Resource cleanup. 10/16/2025 10/16/2025 https://cloudjourney.awsstudygroup.com/ Fri - Learn Amazon ElastiCache + ElastiCache for Redis: Clusters + ElastiCache nodes + ElastiCache for Redis shards - Practice: + Create IAM User Access Keys to configure AWS CLI + Use AWS Console and CLI to create ElastiCache cluster + Use AWS SDK to write and read data on ElastiCache: + Create and connect to Clusters + Set and Get strings + Set and Get a hash with multiple items + Publish (write) and subscribe (read) + Write and read from a stream 10/17/2025 10/17/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in Week 6: Understanding of Relational Databases: Amazon RDS \u0026amp; Aurora Grasped RDS architecture and database engines (MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, Aurora). Understood the Multi-AZ Deployment mechanism. [Image of AWS RDS Multi-AZ architecture]\nLearned how to optimize reads using Read Replicas.\nPractical RDS System Deployment\nSelf-deployed an architecture including: VPC, Subnet Groups, Security Group, EC2 Bastion Host, RDS Private Instance. Connected EC2 to RDS via security groups. Deployed an application connecting to RDS. Performed backup/restore and resource cleanup. Understood and used Amazon DynamoDB (NoSQL)\nUnderstood core concepts: Tables, Items, Attributes, Partition key, Sort key.\nRCU/WCU, On-demand vs Provisioned.\nGlobal Secondary Index (GSI), Local Secondary Index (LSI).\nLearned how to optimize queries through proper key design. Practiced CRUD operations, Query, Scan, and GSI Query. Able to use AWS SDK for Python (Boto3)\nConfigured AWS CLI \u0026amp; credentials to work with Boto3. Wrote Python code to: Create DynamoDB tables. Write / read / update / delete data. Scan data and Perform Conditional Queries. Delete tables and release resources. Understood Boto3 API structure and error handling. Understood and deployed Amazon ElastiCache for Redis\nGrasped ElastiCache architecture: node, shard, cluster, primary/replica.\nUnderstood cluster mode enabled / disabled.\nCreated ElastiCache Redis cluster using Console and CLI.\nUsed AWS SDK to:\nSet/Get key-value. Set/Get hash. Publish/Subscribe. Work with Redis Streams. Understood the role of caching in application optimization.\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Grasp the architecture and operation of the Serverless model with Lambda and API Gateway. Understand the operational mechanism of AWS Lambda and the basics of Event-Driven architecture. Know how to use SQS and SNS with a logical architecture. Configure API Gateway as a Lambda trigger. Save data from Lambda to DynamoDB. Tasks to be implemented this week: Day Task Start Date End Date Resources Mon - Learn about AWS Lambda + IAM Role for Lambda + Lambda function - Learn about Amazon API Gateway + Methods + CORS - Practice: + Create IAM role with least privilege for Lambda to access S3 and DynamoDB + Create Lambda function to handle file uploads + Configure API Gateway to trigger Lambda + Save data from Lambda to DynamoDB 10/20/2025 10/20/2025 https://cloudjourney.awsstudygroup.com/ Tue - Practice: Interaction between Lambda, S3, and DynamoDB + Create image processing Lambda function + Create S3 Bucket + Create IAM Policy for Lambda function and test activity + Create and manage table in AWS DynamoDB + Create Lambda function to write data + Resource cleanup 10/21/2025 10/21/2025 https://cloudjourney.awsstudygroup.com/ Wed - Practice: Host static website on S3 + Create Bucket, enable hosting, assign policy, and upload frontend folder to S3 + Create DynamoDB table, deploy Lambda function to read, write, and delete data + Setup API Gateway + Test API with Postman + Test API with front-end + Resource cleanup 10/22/2025 10/22/2025 https://cloudjourney.awsstudygroup.com/ Thu - Learn Event-driven architecture with Amazon SQS and SNS + SQS: queue\n+ SNS: Pub and Sub - Practice: + Create Queue and SNS Topic + Create API and Lambda function to interact with Queue and SNS Topic + Test activity with APIs + Resource cleanup 10/23/2025 10/23/2025 https://cloudjourney.awsstudygroup.com/ Fri - Learn AWS Step Functions + Orchestrate microservices 10/24/2025 10/24/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in Week 7: Understanding of Serverless Architecture on AWS Grasped the architecture and operating model of AWS Lambda, execution lifecycle, and duration-based pricing. Clearly understood API Gateway acting as a HTTP front-door for Lambda, including: REST API vs HTTP API: Types of APIs in API Gateway used to expose backends; REST API is feature-rich, HTTP API is lightweight and cost-optimized. Methods: Defining HTTP actions (GET/POST/PUT/DELETE) and linking to Lambda or other backends. CORS: Allowing front-ends from different domains to call the API, configured via CORS headers. Integration request/response: The data transformation layer between API Gateway and backend, including input and output mapping. [Image of AWS API Gateway triggering Lambda function architecture]\nUnderstood how to design event-driven architectures, data flows, and advantages over traditional models.\nDeployed Lambda – API Gateway – DynamoDB – S3\nCreated IAM Role with Least Privilege for Lambda to access S3 and DynamoDB. Created Lambda Functions to: Handle file uploads from S3. Read, write, and update DynamoDB data. Created API Gateway to trigger Lambda and handle requests/responses. Configured static website hosting on S3 and connected the front-end with API Gateway. Interactions between Lambda – S3 – DynamoDB\nCreated S3 bucket and enabled event notifications for Lambda. Wrote Lambda for processing images or data uploaded to S3. Performed CRUD operations with DynamoDB from Lambda. Created DynamoDB tables, indexes (if needed), and handled various data formats. Understood and deployed Event-driven systems with SNS \u0026amp; SQS\nGrasped the models:\nSNS (Publisher → Topic → Subscriber) SQS (Producer → Queue → Consumer) Created SNS topics and SQS queues, connected SNS → SQS (fan-out pattern).\nWrote Lambda functions to publish and consume messages.\nTested communication flow via API Gateway and Lambda.\nLearned AWS Step Functions\nUnderstood the concept of orchestration versus choreography. Learned how Step Functions coordinates multiple Lambdas/microservices via a State Machine. "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Automate resource creation using CloudFormation. System configuration management. Tasks to be implemented this week: Day Task Start Date End Date Resources Mon - Learn AWS CloudFormation, Cloud9 + Template: Json, Yaml + Stack + Drift Detection\n-Practice + Use Cloud9 to create a basic CloudFormation template 10/27/2025 10/27/2025 https://cloudjourney.awsstudygroup.com/ Tue - Practice: Advanced CloudFormation: + Create Lambda Function + Create Stack + Connect EC2 instance + Mappings and StackSets + Drift Detection + Resource cleanup 10/28/2025 10/28/2025 https://cloudjourney.awsstudygroup.com/ Wed - Learn AWS Systems Manager (SSM) + Patch Manager + Run Command + Session manager - Practice + Create EC2 instance, IAM Role and assign IAM Role + Configure Patch Manager + Run Command + Resource cleanup 10/29/2025 10/29/2025 https://cloudjourney.awsstudygroup.com/ Thu - Learn AWS CDK Practice: AWS CDK with VS Code + Create public EC2 instance + Configure VS Code environment to use AWS CDK + Create ECS cluster, Application, and Combine API Gateway with Application Load Balancer + Create Lambda function, API Gateway, S3 + Deploy Stack and upload files to S3 + Create Nested Stacks with CDK and deploy + Resource cleanup. 10/30/2025 10/30/2025 https://cloudjourney.awsstudygroup.com/ Fri - Practice: Session Manager: + Create private \u0026amp; public EC2 instances and IAM Role + Connect to Public server + Create connection to Private EC2 server + Update IAM Role to access S3 + Create S3 Buckets and S3 Gateway VPC Endpoint + Monitor session logs with SSM Session Manager + Configure Port Forwarding + Resource cleanup. 10/31/2025 10/31/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in Week 8: Understood and practiced CloudFormation basics\nUnderstood CloudFormation architecture, how templates work, and the Stack lifecycle.\nDistinguished main components:\nTemplate: Defines resources (YAML/JSON). Parameters, Mappings, Resources, Outputs. Stack: A group of resources deployed from a template. StackSets: Deploy stacks across multiple accounts/regions. Drift Detection: Checks for differences between actual resources and the template configuration. Created basic CloudFormation templates to deploy resources (EC2, Lambda, Security Group…).\nDeployed Advanced CloudFormation\nCreated a stack to deploy Lambda functions and EC2. Learned how to use Mappings, Parameters, and Outputs in templates. Used StackSets to deploy to multiple Regions/AWS Accounts. Practiced Drift Detection to detect changes compared to the template. Practiced the basic deploy – update – delete stack workflow. Know how to use AWS Systems Manager (SSM)\nCreated EC2 and configured IAM Role to connect with SSM.\nUsed Session Manager to connect to EC2 without needing SSH/Keypairs.\nPractice:\nRun remote commands with SSM Run Command. Configure and run Patch Manager to update the OS. Record EC2 access logs to S3. Practice Port Forwarding via SSM. Understood security benefits when not opening port 22.\nLearned AWS CDK (Cloud Development Kit)\nUnderstood how CDK allows defining cloud infrastructure using familiar programming languages.\nPracticed creating and deploying Nested Stacks.\nKnow how to use Session Manager\nConnected Public EC2 → Private EC2 via Session Manager. Created and used S3 Gateway VPC Endpoint so Private EC2 can access S3 without Internet. Monitored activities in Session Manager Logs. Cleaned up resources. "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Goals: Get familiar with the AWS data and Machine Learning ecosystem. Understand the key features supported in AWS\u0026rsquo;s AI ecosystem. Tasks to complete this week: Day Tasks Start Date End Date Reference 2 - Learn about AWS Glue + Crawler - Learn about Amazon Athena - Learn about Amazon QuickSight - Learn about Amazon SageMaker 11/03/2025 11/03/2025 https://cloudjourney.awsstudygroup.com/ 3 - Hands-on: Data Analysis + Create IAM Role and IAM Policy + Create S3 Bucket + Create Glue Crawler and validate data + Create notebook with AWS Glue Studio + Analyze using Athena and visualize with QuickSight + Clean up resources 11/04/2025 11/04/2025 https://cloudjourney.awsstudygroup.com/ 4 - Hands-on: Data analysis with Amazon SageMaker + Create SageMaker Studio + Prepare dataset, analyze data, and export to S3 + Train and fine-tune Machine Learning models + Deploy and evaluate model performance + Automatic model tuning + Clean up resources 11/05/2025 11/05/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about Amazon Bedrock + Foundation Models + Bedrock Agents + Knowledge Bases + Bedrock Inference Features 11/06/2025 11/06/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about Pre-trained AI Services + Amazon Rekognition + Amazon Translate + Amazon Textract + Amazon Transcribe + Amazon Polly + Amazon Comprehend + Amazon Kendra + Amazon Lookout + Amazon Personalize 11/07/2025 11/07/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Understand and build a serverless data processing pipeline:\nUnderstand the modern Data Lake model on AWS, with S3 as the central storage. AWS Glue: Glue Crawler automatically detects schema from raw data (CSV/JSON in S3) and generates metadata. Convert unstructured data into structured tables in Glue Data Catalog for querying. Amazon Athena: Query data stored in S3 using SQL without provisioning servers or loading data into a traditional database. Amazon QuickSight: Connect to Athena for data visualization, create BI dashboards to enable data-driven decisions. Understand the Machine Learning development workflow on Amazon SageMaker:\nEnvironment: SageMaker Studio. Training: Understand the decoupled resource model—Notebook for coding, but when calling fit(), SageMaker provisions a separate training cluster, which shuts down automatically after completion to optimize cost. Optimization: Use Automatic Model Tuning to find the best hyperparameters instead of manual experimentation. Deployment: Deploy the trained model to a real-time inference Endpoint (REST API). Grasp the Generative AI trend with Amazon Bedrock:\nUnderstand the shift from \u0026ldquo;training your own model\u0026rdquo; to \u0026ldquo;using Foundation Models (FMs)\u0026rdquo; via API. Understand core components for building modern LLM applications: Knowledge Bases: RAG (Retrieval-Augmented Generation) mechanism to provide enterprise-specific data to AI. Agents: Allow AI not only to \u0026ldquo;chat\u0026rdquo; but also to perform actions (call APIs, retrieve data). Understand the ecosystem of Pre-trained AI Services (no ML model building required):\nClassify service groups to apply to the right business problem without building models from scratch:\nVision: Rekognition (image/video analysis). Speech/Audio: Transcribe (STT), Polly (TTS). NLP/Text: Translate, Comprehend (sentiment/entity analysis), Textract (document OCR). Specialized: Personalize (recommendations), Lookout (predictive maintenance). "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Welcome to Bao Nguyen’s WorkShop\nWelcome to my worklog – a personal journey with AWS \u0026ldquo;The First Cloud Journey\u0026rdquo;.\nThis space captures my 12-week path of learning, experimenting, and building with Amazon Web Services.\nFrom the very first steps of exploring the fundamentals to diving into hands-on projects, this worklog reflects how I started my journey ****“into the cloud”**. It is not only a record of tasks and achievements, but also a story of growth, challenges, and discoveries along the way.\nLet’s go!\nWeek 1: Getting started with AWS, account setup, and cost management\nWeek 2: Building network infrastructure and security with Amazon VPC\nWeek 3: Managing EC2 virtual servers, Auto Scaling, and system monitoring\nWeek 4: Data storage and backup/disaster recovery solutions on AWS\nWeek 5: System security, Identity and Access Management (IAM), and compliance\nWeek 6: Deploying and operating databases (RDS, DynamoDB, ElastiCache)\nWeek 7: Serverless architecture and Event-Driven models on AWS\nWeek 8: Infrastructure Automation (IaC) and system configuration management\nWeek 9: Data analytics and the AI/Machine Learning ecosystem on AWS\nWeek 10: System monitoring, auditing, and application deployment with AWS Amplify\nWeek 11: CI/CD processes, application deployment with Elastic Beanstalk, and AI service integration\nWeek 12: Knowledge consolidation, system design, and technical preparation for the Capstone Project\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/5-workshop/5.4-s3-onprem/5.4.2-create-agent/",
	"title": "Create Agent",
	"tags": [],
	"description": "",
	"content": " On the interface page of Aws Bedrock, I choose the left side Agent Please fill in the name and architecture for the Agent to help it understand the context For example, I write it as an Aws consultant. But before I can get that answer, I have to load Knowledge base for it to understand - Content of Knowledge base Note that you should configure it according to your needs and should grant it the necessary permissions\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "1. Create an IAM Role using the Console with the required permissions Create an IAM role named bedrock and assign the necessary permissions to use Amazon Bedrock: model/agent invocation permissions (bedrock:InvokeModel, bedrock:CreateAgent, bedrock:InvokeAgent), S3 access permissions (read/write) to load documents to the Knowledge Base, and CloudWatch Logs permissions to write logs, etc.\nDocument templates to load kwoledge base (PDF, text, markdown, HTML) We will create an introduction document to Amazon Web Services (Text) Amazon Web Services (AWS) is Amazon\u0026#39;s cloud computing platform, officially launched in 2006. Initially, AWS was created to solve Amazon\u0026#39;s internal problem: they needed a flexible scalable infrastructure to serve their huge e-commerce system. When realizing that other businesses were also facing similar difficulties with IT infrastructure, Amazon decided to commercialize this platform and provide it as a cloud service. AWS was born with the goal of helping organizations and businesses reduce server investment costs, deploy applications faster, and take advantage of infrastructure on demand instead of having to operate data centers themselves. This is the core vision behind the modern \u0026#34;cloud computing\u0026#34; model: turning computing resources into a utility service that can be used immediately, like electricity and water. In the following years, AWS grew rapidly, from a few basic services such as storage (Amazon S3) and virtual machines (EC2) to an ecosystem of more than 200 services including virtual servers, containers, artificial intelligence, data storage, security, IoT and data analytics. Thanks to its global scalability with dozens of Regions and hundreds of Edge points around the world, AWS became the leading cloud platform in the market. AWS\u0026#39;s long-term goal is to provide a secure, flexible computing environment that can meet the needs of all users — from small startups to large corporations. AWS aims to help businesses focus on product development, instead of spending time operating infrastructure. To date, AWS is used by leading technology companies, governments, universities and millions of organizations around the world. AWS is not just an infrastructure service, but has also become an important platform to drive innovation in areas such as machine learning, Big Data, generative AI with (Amazon Bedrock), and serverless application development. Thanks to that, AWS plays an important role in shaping the modern technology generation. "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "FoodMind Recommender Platform For Prompt-IPoG An integrated AWS Serverless solution for personalized meal tracking and recommendations. 1. Executive Summary FoodMind Recommender Platform is an intelligent web platform designed to serve as a personalized eating assistant. It automatically calculates the user\u0026rsquo;s Total Daily Energy Expenditure (TDEE) based on their profile and leverages AWS Bedrock (Generative AI) to allow users to log meals in natural language (e.g., “I just ate a bowl of beef pho”).\nThe system also provides intelligent meal recommendations (Breakfast/Lunch/Dinner) by automatically filtering dishes according to calorie goals and health constraints (e.g., “allergies”, “gout”).\nThe entire solution is built on a serverless architecture with AWS Amplify (Frontend), API Gateway, AWS Lambda, and Amazon DynamoDB (Backend). It enables users to receive meal recommendations based on their calculated calorie intake for the day. Data is stored and queried through Amazon DynamoDB, ensuring high performance and flexible scalability.\nThe solution emphasizes the combination of AI and real-world data to support smart meal decision-making and offers a dashboard for users to track their eating habits effectively.\n2. Problem Statement Current Problem\nMany users struggle to manage their daily diet — they don’t know how many calories to consume or which foods suit their daily health goals. Manual logging and nutritional lookup are time-consuming, inaccurate, and not personalized.\nSolution\nFoodMind Recommender Platform applies AI and AWS Cloud to automate the entire meal tracking and recommendation process:\nGoal Automation: Automatically calculates calorie targets (using the Mifflin-St Jeor formula) when users update their profiles.\nRecommendation Automation: Provides a GET /recommendations API using Lambda business logic to filter dishes from the database based on calorie targets (e.g., lunch \u0026lt; 700 calories) and health restrictions (e.g., avoid “red meat” for gout).\nAI Logging Automation: Provides a POST /log-food API using AWS Bedrock to parse natural language input, extract calories, and store logs.\nSelf-learning Automation: When users log a new dish (e.g., “bun dau mam tom”) unknown to the system, AWS Bedrock estimates its calories and automatically adds it to the knowledge base.\nVisual Tracking: The dashboard (Amplify) displays a 7-day meal history, helping users monitor and manage their dietary habits.\nUsers simply input data — the system understands, analyzes, and recommends suitable meals tailored to personal health goals.\nBenefits and ROI\nSaves time on nutrition tracking, eliminates manual work. Offers highly personalized AI experience. Creates a structured dataset for AI research in food and nutrition. Low cost thanks to serverless architecture. Scalable and reusable for other health-related applications. ROI estimate: payback within 6 months through reduced development time and model reuse. Estimated cost: around 10–15 USD/month. 3. Solution Architecture The platform is fully built on AI-as-a-Service combined with AWS Serverless, ensuring high performance, security, and scalability. Nutritional data on dishes is stored in Amazon DynamoDB, which supports generating meal recommendations based on calculated calorie targets. Amazon Bedrock processes user natural language to extract calorie information and log meals in DynamoDB. AWS Amplify hosts the Next.js web interface, and Amazon Cognito ensures secure user authentication. The architecture is detailed below:\nAWS Services Used\nAWS Amplify: Deploys and hosts the web interface (Next.js), integrates with GitLab CI/CD for auto build and deploy. Amazon Route 53 + AWS WAF + Amazon CloudFront: Edge layer for secure and fast content delivery worldwide. Amazon Cognito: Manages user authentication, login, and access control. Amazon API Gateway: Provides endpoints for GET /Recommendation, POST /Log, GET /Dashboard, connected to Lambda. AWS Lambda (Private Subnet): Handles business logic, calls Bedrock and DynamoDB via VPC Endpoints for security. AWS Bedrock: Generates dish descriptions, normalizes meal logs, and stores them in DynamoDB for personalized recommendations. Amazon DynamoDB: Stores user data, meal logs, calorie goals, and recommendation data with scalable performance. AWS Secrets Manager: Secures credentials (API Keys, Bedrock access) for Lambda and backend. Amazon CloudWatch \u0026amp; AWS CloudTrail: Monitors logs, access, and performance; supports incident recovery. Amazon S3: Stores system logs and backups. AWS IAM: Manages detailed access permissions between services and users. Amazon VPC: Isolates Lambda in a private subnet to ensure secure internal communication between Lambda, DynamoDB, and Bedrock. Component Design\nUser Management: Amazon Cognito controls user access. Content Delivery \u0026amp; Security: Route 53 routes domain, WAF prevents web attacks (SQL Injection, DDoS), CloudFront speeds up global delivery. Web Interface: Amplify hosts the Next.js app. Meal Logging \u0026amp; Recommendation: User input (text) stored in DynamoDB; Lambda recommends dishes based on calorie calculations. Meal Analysis: Lambda calls Bedrock to process user input, extract calorie data, and log new dishes in DynamoDB if missing. Dashboard Display: Amplify shows calorie charts by day/week/month. Authentication \u0026amp; Security: Cognito ensures secure login and user management. Monitoring \u0026amp; Tracking: CloudWatch monitors logs and Lambda performance; CloudTrail records API and user activity history. 4. Technical Implementation Implementation Phases\nResearch \u0026amp; Design: Build AI + Cloud pipeline, check feasibility, and design AWS architecture (Weeks 1–5). Cost Optimization: Validate AWS service pricing to optimize budget (Week 6). Development \u0026amp; Deployment: Load initial data, build Next.js frontend, test APIs, and deploy final product (Weeks 7–11). Technical Requirements\nCalorie Dataset: Collect initial data and load into DynamoDB using AWS SDK (Boto3). Recommendation Platform: Requires knowledge of AWS Amplify (Next.js hosting), S3, Cognito, and Serverless stack (Lambda, DynamoDB, API Gateway), DynamoDB schema (PK, SK), and Bedrock API integration. 5. Roadmap \u0026amp; Milestones Internship (Jan–Mar): January: Learn and master AWS services. February: Design and refine architecture. March: Deploy, test, and launch the system. Post-deployment: Maintain, enhance recommendations, and expand data within one year. 6. Budget Estimation Cost reference: AWS Pricing Calculator\nOr download budget estimation file.\nInfrastructure Cost\nAWS Amplify: 0.50 USD/month (~100 MB, low traffic). AWS Lambda: 0.20–0.30 USD/month (100,000 requests/month, avg runtime \u0026lt;1s). Amazon API Gateway: 0.10–0.20 USD/month (50,000 REST API requests/month). Amazon DynamoDB (Paid Plan): ~0.30 USD/month (50 MB data, ~20,000 requests/month, On-Demand). Amazon S3 (log/backup): 0.10 USD/month (\u0026lt;2GB). AWS Bedrock: 3.00–5.00 USD/month (a few thousand tokens/month). CloudWatch + CloudTrail + IAM: ~0.10 USD/month. Amazon Cognito: 0.00 USD/month (\u0026lt;50 active users, Free Tier). Total: ~4–6 USD/month (~50–75 USD/year).\n7. Risk Assessment Risk Matrix\nAI misinterpretation — High impact, Low probability. API overload — Medium impact, Low probability. Budget overrun — Medium impact, Low probability. Logic error — Medium impact, Low probability. Mitigation Strategies\nAI deviation: Careful prompt engineering. API overload: Request throttling via API Gateway. Budget: AWS budget alerts and service optimization. Logic: Rigorous Lambda testing and validation. Contingency Plan\nManual data collection fallback if AWS outage occurs. Use CloudFormation to restore infrastructure configurations. 8. Expected Outcomes Enhanced User Experience: Provides a smart meal assistant, removing manual effort in tracking and choosing meals.\nPractical AI Integration: Demonstrates real-world AWS Bedrock integration in production-level systems.\nNutrition Data Foundation: Expandable dataset for AI and healthcare research.\nScalability: Extendable to image-based food analysis, AI chat coaching, and mobile applications.\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Know how to deploy Amazon CloudWatch to monitor the system. Understand the AWS CloudTrail process for recording detailed API activity tracking. Tasks to be implemented this week: Day Task Start Date End Date Resources Mon - Learn about Amazon CloudWatch + Metrics + Logs + Alarms + Events + Dashboards + AWS X-Ray 11/10/2025 11/10/2025 https://cloudjourney.awsstudygroup.com/ Tue - Practice: + Create IAM Role, IAM policy, and configure EC2 Instance + Configure CloudWatch metrics, logs + Activate CloudWatch Alarms + Configure Dashboards + Resource cleanup 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ Wed - Learn AWS CloudTrail + Trails + Events: Read/Write/All + CloudTrail Insights 11/12/2025 11/12/2025 https://cloudjourney.awsstudygroup.com/ Thu - Learn AWS Amplify + Frontend + Backend + Storage + Authentication 11/13/2025 11/13/2025 https://cloudjourney.awsstudygroup.com/ Fri - Practice:: Monitor Lambda with CloudWatch and X-Ray + Host source code on Amplify + Monitor with CloudWatch: Debug with logs, create customer metrics, and create alerts with Alarms + Monitor with X-Ray + Resource cleanup. 11/14/2025 11/14/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in Week 10: Basic deployment of comprehensive monitoring system with Amazon CloudWatch:\nMetrics: Distinguished between Default Metrics (like CPU, Network) and Custom Metrics (like Memory usage, Disk space - metrics AWS cannot automatically collect from outside the hypervisor). Knew how to install CloudWatch Agent to push these custom metrics from EC2 to CloudWatch. Centralized Log Management: Knew how to aggregate logs from multiple sources (EC2 app logs, Lambda execution logs) into CloudWatch Logs Groups for lookup. Alarms: Established an alert system (via email/SMS with SNS) when the system encounters issues (CPU overload, full disk, or application returning many errors). Management Dashboard: Built a visual dashboard displaying the health of the entire system on a single screen. Security control with AWS CloudTrail:\nClearly distinguished the core difference: CloudWatch answers \u0026ldquo;How is the system performing?\u0026rdquo;, while CloudTrail answers \u0026ldquo;Who did what to the system?\u0026rdquo;. Knew how to track API call history for security and incident investigation purposes. Example: Finding out who accidentally deleted a Security Group or who stopped an EC2 instance. Understood CloudTrail Insights to automatically detect anomalous behaviors in the account (e.g., a spike in API calls to create resources). Rapid application deployment with AWS Amplify:\nUnderstood Amplify is a toolkit to accelerate the web/mobile app development process, automating the connection of Frontend to Backend services (Auth, Storage, API).\nSuccessfully practiced a simplified CI/CD process: Push code to Git -\u0026gt; Amplify automatically builds and deploys to the Hosting environment.\nKnew how to integrate monitoring (CloudWatch/X-Ray) directly into the application deployed by Amplify to track end-user experience.\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Understand the CI/CD process and the AWS Code Series toolset. Experience Amazon AI services. Tasks to implement this week: Day Task Start Date End Date Resources Mon - Explore CI/CD orchestrator services on AWS + AWS CodeCommit + AWS CodeBuild + AWS CodeDeploy + AWS CodePipeline 11/17/2025 11/17/2025 https://cloudjourney.awsstudygroup.com/ Tue - Explore AWS Elastic Beanstalk + Application + Environment - Hands-on: Deploy a web application on AWS with Elastic Beanstalk + Create CloudFormation stack + Connect Linux EC2 instance and install database + Check local server to download project + Deploy on Elastic Beanstalk + Update application + Check environment and query EC2 instance info + Clean up resources. 11/18/2025 11/18/2025 https://cloudjourney.awsstudygroup.com/ Wed - Update knowledge on Amazon Bedrock + Better understand pre-trained AI services like cost and hơw to call API 11/19/2025 11/19/2025 https://cloudjourney.awsstudygroup.com/ Thu - Hands-on: Use Amazon Polly + Setup DynamoDB + Explore Amazon Polly + Create Speech and speech marks using AWS CLI + Create speech marks using AWS Polly SDK for Java 11/20/2025 11/20/2025 https://cloudjourney.awsstudygroup.com/ Fri - Hands-on: Use Amazon Rekognition + Create Cognito Identity Pool - Object detection with Amazon Rekognition + Facial recognition + Test person finding application + Attach EBS volume 11/21/2025 11/21/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in Week 11: Understand the \u0026ldquo;Software Supply Chain\u0026rdquo; process on AWS (CI/CD):\nUnderstood the shift from manual deployment (copying files, SSH into server) to a fully automated process with the AWS Code Series.\nGrasped the role of each service:\nAWS CodeCommit: Source code version control (Git-based), the origin of all changes. AWS CodeBuild: Automated compilation and testing environment (Unit Test), ensuring \u0026ldquo;clean\u0026rdquo; code before proceeding. AWS CodeDeploy: Automates code deployment to server fleets, minimizing downtime and human error. AWS CodePipeline: The \u0026ldquo;Conductor\u0026rdquo; coordinating the entire process above into a continuous flow. Grasped Platform as a Service with Elastic Beanstalk:\nUnderstood the value of using Elastic Beanstalk to handle the entire configuration process instead of manually configuring VPC, EC2, Load Balancer, and Auto Scaling Groups. Updated strategic vision on Generative AI via the event: Generative AI with Amazon Bedrock\nUnderstood the model shift from \u0026ldquo;Self-training models\u0026rdquo; to \u0026ldquo;Using Foundation Models (FMs) via API\u0026rdquo;. Grasped core Amazon Bedrock concepts: Serverless AI, ability to integrate Knowledge Bases (RAG) and Agents to build secure and private enterprise AI applications. In-depth hands-on integration of AI Services:\nWith Amazon Polly: Converting text to audio Know how to convert simple text to speech. Know how to handle Speech Marks (Metadata about the real-time timing of words), a technique for lip-syncing virtual characters. Know how to use the SDK (Java) to integrate Polly into the backend. With Amazon Rekognition: Know how to use Cognito Identity Pool to grant temporary permissions for applications to access Rekognition directly without hard-coding Access Keys. Understood how to use the API to solve complex computer vision problems: Facial recognition, object detection, and person identification. "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Review Serverless and AI knowledge systems. Detailed design of DynamoDB database tables for the project and API. Detailed check of connections between services in the project. Tasks to implement this week: Day Task Start Date End Date Resources Mon - Review and design database (DynamoDB) + Review knowledge on Partition Key (PK) and Sort Key (SK) - Hands-on + Design Schema for project tables + Determine Access Patterns (Data query methods) for the project. 11/24/2025 11/24/2025 https://cloudjourney.awsstudygroup.com/ Tue - Review and design API (API Gateway + Lambda): + Review how to create REST API, Lambda Proxy Integration. + List necessary API endpoints: POST, GET 11/25/2025 11/25/2025 https://cloudjourney.awsstudygroup.com/ Wed - Review AI integration flow with Bedrock + Review Bedrock API call methods + Review basic Prompt Engineering 11/26/2025 11/26/2025 https://cloudjourney.awsstudygroup.com/ Thu - Review security and authentication (Cognito + IAM ) + Review User Login flow -\u0026gt; Receive Token -\u0026gt; Send Token to API Gateway. + Review IAM Policy regarding least privilege 11/27/2025 11/27/2025 https://cloudjourney.awsstudygroup.com/ Fri - Review system monitoring and operational methods + Review CloudWatch to plan monitoring when the project runs. + Review Boto3 + Plan project deployment steps 11/28/2025 11/28/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in Week 12: Completed Schema Design for DynamoDB:\nDesigned specific data tables, optimized for data retrieval. Clearly defined connection interfaces:\nDefined clear input/output for Lambda functions. Grasped how to use the boto3 library to connect AWS \u0026ldquo;puzzle pieces\u0026rdquo; together. Prepared AI scenarios (Prompt Strategy) ready to test Bedrock as expected.\nUnderstood the Identity flow from Cognito through API Gateway down to Lambda.\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/5-workshop/5.3-vpc-to-internet/",
	"title": "Exploring Bedrock Console",
	"tags": [],
	"description": "",
	"content": " In this section, you will explore Amazon Bedrock Console and run quick tests in Playground to evaluate both the quality and cost of your models. Before selecting a model for production, measure the average token count, compare responses between models, and estimate costs to choose the model that fits your budget. Contents Exploring Bedrock "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "During the process of learning and exploring AWS services, I have also translated several blogs to enhance my knowledge:\nBlog 1 - How a Customer Reduced Total Cost of Ownership (TCO) by 28% for Storage with Amazon FSx for NetApp ONTAP This blog explains how a multi-branch organization optimized its storage using Amazon FSx for NetApp ONTAP. The architecture leverages FlexCache, SnapMirror, and Multi-AZ to improve access performance, simplify operations, and reduce total cost of ownership (TCO) by 28% compared to on-premises solutions.\nBlog 2 - Automating Vector Embedding Generation in Aurora PostgreSQL with Bedrock This blog guides on how to automatically generate vector embeddings for PostgreSQL data using Aurora + pgvector + Bedrock. It presents multiple strategies (triggers, synchronous/asynchronous Lambda, SQS batch processing, pg_cron), analyzing their pros and cons in terms of latency, synchronization, scalability, and provides guidance on designing an optimal embedding pipeline for AI applications.\nBlog 3 - Grouping Database Tables under AWS Database Migration Service Tasks for PostgreSQL Source Engine This blog explains how to analyze PostgreSQL metadata and table characteristics to group tables effectively into AWS DMS tasks. It guides on load balancing, reducing CDC latency, handling large tables, LOB columns, or tables without PK/UK, and shares best practices for performing large-scale database migrations efficiently.\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/5-workshop/5.4-s3-onprem/",
	"title": "Agent Design",
	"tags": [],
	"description": "",
	"content": "Overview In this section, we will learn how to design an AI Agent that operates on the Amazon Bedrock platform. The main goal of the chapter is to help you understand what an Agent is, how it handles user requests, and why Agents play an important role in building modern AI applications.\nIn this overview, we will outline the important components of the Agent design process, including how to define the Agent\u0026rsquo;s tasks, describe the desired behavior, structure the workflow, choose the Foundation Model, as well as the principles to follow for the Agent to operate stably and accurately. This section also introduces how Agents interact with enterprise data and integrate with AWS services for deployment in production environments.\nContents Create Knowledge base Create Agent "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Cloud Day\nDate \u0026amp; Time: 9:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Generative AI with Amazon Bedrock\nDate \u0026amp; Time: 08:30, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Building an AI Agent with AWS Bedrock Overview Amazon Bedrock is a fully managed AI/ML service provided by AWS, allowing businesses to easily access and utilize foundational models such as Claude, Llama, Stable Diffusion, Amazon Titan, and others without the need to build complex AI infrastructure.\nWe can call these models through the Bedrock API or Bedrock console without having to deploy AI infrastructure ourselves.\nContent Workshop Overview Preparing the AWS Environment Building a Basic AI Agent Creating API Gateway for the AI Agent Testing and Deployment Cleaning Up Resources "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/5-workshop/5.6-cleanup/",
	"title": "Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "Cleanup Resources Congratulations on completing this workshop!\nIn this workshop, you learned about AI Agent with bedrock on Aws and tried it out with python (boto3).\nCleanup Navigate to AWS Bedrock on the left side of the dashboard. Select Agent. Delete and confirm the deletion by typing \u0026ldquo;delete\u0026rdquo;. Navigate to AWS Bedrock on the left side of the dashboard. Select Knowledge Base .Delete and confirm the deletion by entering the keyword \u0026ldquo;delete\u0026rdquo; Delete the unused IAM Role "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services Vietnam Co., Ltd. from 09/08/2025 to 12/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in AWS First Cloud Journey (FCJ) involves learning and exchanging knowledge with peers and mentors, as well as attending AWS events., through which I improved my skills in programming, analysis, blog translation, report writing, communication, best practices with basic AWS services..\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Improve discipline and initiative in communicating with people about work Need to improve and enhance problem-solving thinking, develop presentation and public speaking skills Learn to communicate better in daily communication and at work, handle situations "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "I would like to share some personal thoughts about my experience participating in the First Cloud Journey program, based on what I have gone through during this challenging new journey. The program has helped me learn and experience many interesting and valuable insights.\nGeneral Evaluation 1. Working Environment The working environment is very friendly and open. FCJ members are always ready to support me when I face difficulties, even outside of working hours. The workspace is organized and comfortable, helping me focus better. However, I think adding some social sessions or knowledge-based games during the learning process would help everyone understand each other better and learn from one another more effectively.\n2. Support from Mentors / Team Admin The mentors provide very detailed guidance, explaining clearly when I don\u0026rsquo;t understand, and always encouraging me to ask questions. The Team Admin supports administrative procedures and documentation, creating favorable conditions for me to work. The mentors do not just answer technical questions but also guide my problem-solving mindset. I highly appreciate that the mentors allow me to try and handle problems myself instead of just giving the answers.\n3. Alignment between Work and Major As an AI major student, participating in FCJ and diving deep into AWS Cloud is a perfect complement to my career path. The assigned work not only helps reinforce foundational knowledge of Programming/Data but also opens up a new mindset regarding infrastructure and real-world application deployment.\n4. Opportunities for Learning \u0026amp; Skill Development During the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. Mentors also shared a lot of practical experience to help me better orient my career. The real-world experiences shared by Mentors are invaluable lessons that schools can hardly provide enough of.\n5. Culture \u0026amp; Team Spirit The company culture is very positive: everyone respects each other, works seriously but maintains a cheerful atmosphere. When there are urgent projects, everyone strives together, sharing and supporting regardless of position. This makes me feel like part of the collective, even though I am just an intern.\n6. Policies / Benefits for Interns The company has very fair support policies for interns, from allowances to flexibility in working hours to balance studies. In particular, the greatest value I received was participating in internal training sessions and knowledge-sharing workshops.\nOther Questions What satisfied you the most during the internship?\nIt was receiving wholehearted support from the mentors and team members on my journey to the cloud from zero. Although this journey was somewhat difficult, thanks to that sharing and assistance, I gradually improved and learned many new things crucial for my future career. What do you think the company needs to improve for future interns?\nI think there should be a specific roadmap so that interns can research and learn faster; many get lost researching forever and lose direction, not knowing what they are doing. If introducing this to friends, would you recommend they intern here? Why?\nI would introduce my friends. Because this is an ideal environment to transform textbook knowledge into battle-tested skills to deploy ideas into products. If my friends want to truly understand what \u0026ldquo;doing Cloud\u0026rdquo; is like, rather than just theory, FCJ is the best starting point. Suggestions \u0026amp; Wishes Do you have any suggestions to improve the experience during the internship? Do you want to continue this program in the future? I really hope to have the opportunity to continue accompanying the company. Other feedback (feel free to share): I would like to send my sincere thanks to the Mentors and the FCJ Team for patiently and dedicatedly guiding me over the past 3 months. The First Cloud Journey gave me not only knowledge but also the confidence to pursue a professional Cloud \u0026amp; AI Engineering path. "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/5-workshop/5.5-policy/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "title : \u0026ldquo;On-premise testing\u0026rdquo;\nweight : 5 chapter : false pre : \u0026quot; 5.5 \u0026quot; In this section, we will deploy and test the AI ​​model in an on-premise environment. This is an important step to evaluate the feasibility and performance of the system in real conditions before putting it into production 1.\nWe will connect via python (boto3) First we must know our region, Agent ID, Aliases Id\nMust have accesskey Id, secret key: Note that these 2 keys cannot be revealed, otherwise outsiders will use the service and you will lose money\nTry promt to see if it returns a result or not, if it does, it is considered successful. "
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://baoxanhla.github.io/AWS_WORKSHOP_IPoG/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]